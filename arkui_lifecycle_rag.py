"""
ArkUI è‡ªå®šä¹‰ç»„ä»¶ç”Ÿå‘½å‘¨æœŸåˆ†æ RAG ç³»ç»Ÿ
"""

from dotenv import load_dotenv
load_dotenv()

import os
from langchain import hub
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from typing import List

# ==================== é…ç½®éƒ¨åˆ† ====================
CONFIG = {
    "vector_store_path": "./vector_store",
    "input_file": "input.txt",
    "model_name": "deepseek-chat", 
    "temperature": 0,
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "retriever_k": 4  # æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
}

# ==================== æç¤ºè¯æ¨¡æ¿ ====================
PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½ç²¾é€š HarmonyOS ArkTS ç”Ÿå‘½å‘¨æœŸæœºåˆ¶çš„ä¸“å®¶ï¼Œç°éœ€è¦ä» ArkTS ç¤ºä¾‹ä»£ç ä¸­è§£æå¹¶æå–æ‰€æœ‰ç›¸å…³ç”Ÿå‘½å‘¨æœŸå‡½æ•°ã€‚

è¯·å‚é˜…ä»¥ä¸‹å‚è€ƒæ–‡æ¡£å’Œä»£ç ç¤ºä¾‹ï¼š
- å‚è€ƒæ–‡æ¡£ç‰‡æ®µæä¾›äºä¸‹æ–¹ï¼Œæ ‡è®°ä¸º {context}ã€‚
- ArkTS ç¤ºä¾‹ä»£ç æä¾›äºä¸‹æ–¹ï¼Œæ ‡è®°ä¸º {question}ã€‚

ä»»åŠ¡è¦æ±‚å¦‚ä¸‹ï¼š
1. ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•é™„åŠ æ–‡å­—ã€‚
2. å‚è€ƒä»¥ä¸‹ JSON ç»“æ„æ ¼å¼ï¼š
```
{{
  "lifecycle": {{
    "functions": [
      {{
        "name": "å‡½æ•°å",
        "scope": "page æˆ– component",
        "description": "ç®€è¦è¯´æ˜è§¦å‘æ—¶æœºå’Œä½œç”¨"
      }}
    ],
    "order": [
      "å‡½æ•°è°ƒç”¨é¡ºåºï¼ŒæŒ‰æ—¶é—´å…ˆåæ’åˆ—ï¼Œä¾‹å¦‚ Parent.aboutToAppear â†’ Child.aboutToAppear ..."
    ],
    "dynamicBehavior": "è¯´æ˜åŠ¨æ€æƒ…å†µä¸‹ï¼ˆå¦‚æ¡ä»¶æ¸²æŸ“ã€çŠ¶æ€åˆ‡æ¢ï¼‰ç”Ÿå‘½å‘¨æœŸçš„è°ƒç”¨å˜åŒ–"
  }}
}}

è¯´æ˜ï¼š
1. ä¸€ä¸ªfunctionsåˆ—è¡¨ï¼šåŒ…å«æ‰€æœ‰çš„ç”Ÿå‘½å‘¨æœŸå‡½æ•°æ–¹æ³•ï¼Œä¸ç”¨åŒºåˆ†ç»„ä»¶ã€‚
2. ä¸€ä¸ªorderåˆ—è¡¨ï¼šåŒ…å«æ‰€æœ‰çš„ç”Ÿå‘½å‘¨æœŸå‡½æ•°å…ˆåé¡ºåºï¼Œä¸¤ä¸ªä¸ºä¸€ç»„ï¼Œç”¨predå’Œsuccè¡¨ç¤ºã€‚
ä¾‹å¦‚ï¼šå…±æœ‰ä¸‰ä¸ªç”Ÿå‘½å‘¨æœŸå‡½æ•°ï¼ŒonCreate, onResume, onDestory
functions: [onCreate, onResume, onDestory]
order: [{{pred: onCreate, succ: onResume}}, {{pred: onResume, succ: onDestory}}]
```
3. åˆå¹¶å¤šä¸ªç»„ä»¶çš„ç”Ÿå‘½å‘¨æœŸï¼š
- å°† Parentã€Child æˆ–å…¶ä»–ç»„ä»¶ä¸­çš„ç”Ÿå‘½å‘¨æœŸå‡½æ•°ç»Ÿä¸€è¾“å‡ºã€‚
- ä½¿ç”¨ Parent.xxx æˆ– Child.xxx åŒºåˆ†å‡½æ•°æ‰€å±ç»„ä»¶ã€‚
- è¯·ä¾æ®å®é™…æ‰§è¡Œé¡ºåºæ•´ç†è°ƒç”¨é“¾æ¡ï¼Œä»ç»„ä»¶å‡ºç°åˆ°ç»„ä»¶æ¶ˆå¤±ã€‚

4. æç‚¼å¹¶ç®€è¿°æ¯ä¸ªå‡½æ•°çš„è¯´æ˜ä¸è°ƒç”¨æ—¶æœºã€‚

5. è¾“å‡ºæ—¶ä»…æä¾› JSON ç»“æœã€‚
"""


# ==================== æ•°æ®åŠ è½½å‡½æ•° ====================
def load_and_index_pdf(pdf_path: str, persist_dir: str) -> Chroma:
    """åŠ è½½ PDF å¹¶åˆ›å»ºå‘é‡ç´¢å¼•"""
    print(f"ğŸ“š æ­£åœ¨åŠ è½½ PDF: {pdf_path}")
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    print(f"âœ… å·²åŠ è½½ {len(docs)} é¡µæ–‡æ¡£")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CONFIG["chunk_size"],
        chunk_overlap=CONFIG["chunk_overlap"]
    )
    splits = text_splitter.split_documents(docs)
    print(f"âœ… å·²åˆ†å‰²ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")
    
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=OpenAIEmbeddings(),
        persist_directory=persist_dir
    )
    print(f"âœ… å‘é‡ç´¢å¼•å·²ä¿å­˜åˆ°: {persist_dir}")
    return vectorstore


# ==================== åˆå§‹åŒ–å‘é‡åº“ ====================
def init_vectorstore(persist_dir: str) -> Chroma:
    """åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“"""
    embedding = OpenAIEmbeddings()
    
    if not os.path.exists(persist_dir):
        raise FileNotFoundError(
            f"âŒ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {persist_dir}\n"
            "è¯·å…ˆå–æ¶ˆæ³¨é‡Šæ•°æ®åŠ è½½ä»£ç ï¼Œè¿è¡Œä¸€æ¬¡ä»¥åˆ›å»ºç´¢å¼•ã€‚"
        )
    
    vectorstore = Chroma(
        persist_directory=persist_dir,
        embedding_function=embedding
    )
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    try:
        count = vectorstore._collection.count()
        if count == 0:
            raise ValueError("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œè¯·é‡æ–°ç´¢å¼•æ–‡æ¡£")
        print(f"âœ… å‘é‡æ•°æ®åº“å·²åŠ è½½ï¼ŒåŒ…å« {count} ä¸ªæ–‡æ¡£å—")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•æ£€æŸ¥å‘é‡åº“çŠ¶æ€: {e}")
    
    return vectorstore


# ==================== æ„å»º RAG é“¾ ====================
def create_rag_chain(vectorstore: Chroma, model_name: str, temperature: float):
    """åˆ›å»º RAG æ¨ç†é“¾"""
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": CONFIG["retriever_k"]}
    )
    
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=PROMPT_TEMPLATE
    )
    
    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature
    )
    
    def format_docs(docs):
        # æ·»åŠ æ¥æºä¿¡æ¯
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get('source', 'æœªçŸ¥')
            page = doc.metadata.get('page', '?')
            formatted.append(f"[ç‰‡æ®µ {i} - æ¥æº: {source}, é¡µ: {page}]\n{doc.page_content}")
        return "\n\n---\n\n".join(formatted)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain


# ==================== è¯»å–è¾“å…¥æ–‡ä»¶ ====================
def read_input_file(filepath: str) -> str:
    """è¯»å–è¾“å…¥æ–‡ä»¶"""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼š{filepath}")
    
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read().strip()
    
    if not content:
        raise ValueError(f"è¾“å…¥æ–‡ä»¶ä¸ºç©ºï¼š{filepath}")
    
    print(f"âœ… å·²è¯»å–è¾“å…¥æ–‡ä»¶: {filepath}")
    print(f"ğŸ“ å†…å®¹é¢„è§ˆ:\n{content[:200]}{'...' if len(content) > 200 else ''}\n")
    return content


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    print("=" * 60)
    print("ğŸš€ ArkUI ç”Ÿå‘½å‘¨æœŸåˆ†æ RAG ç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–å‘é‡åº“
        vectorstore = init_vectorstore(CONFIG["vector_store_path"])
        
        # 2. è¯»å–è¾“å…¥
        scene_text = read_input_file(CONFIG["input_file"])
        
        # 3. åˆ›å»º RAG é“¾
        print("\nğŸ”— æ­£åœ¨æ„å»º RAG æ¨ç†é“¾...")
        rag_chain = create_rag_chain(
            vectorstore,
            CONFIG["model_name"],
            CONFIG["temperature"]
        )
        
        # 4. æ‰§è¡Œæ¨ç†
        print("ğŸ¤” æ­£åœ¨åˆ†æç”Ÿå‘½å‘¨æœŸè°ƒç”¨é¡ºåº...\n")
        result = rag_chain.invoke(scene_text)
        
        # 5. è¾“å‡ºç»“æœ
        print("=" * 60)
        print("ğŸ“œ ç”Ÿå‘½å‘¨æœŸè°ƒç”¨é¡ºåºåˆ†æç»“æœ")
        print("=" * 60)
        print(result)
        
        # 6. ä¿å­˜ç»“æœï¼ˆå¯é€‰ï¼‰
        output_file = "lifecycle_analysis_output.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(result)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except FileNotFoundError as e:
        print(f"\nâŒ æ–‡ä»¶é”™è¯¯: {e}")
    except ValueError as e:
        print(f"\nâŒ æ•°æ®é”™è¯¯: {e}")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()


# ==================== å¯é€‰ï¼šæ•°æ®ç´¢å¼•è„šæœ¬ ====================
def index_pdf_documents():
    """
    é¦–æ¬¡è¿è¡Œæ—¶ä½¿ç”¨æ­¤å‡½æ•°åˆ›å»ºå‘é‡ç´¢å¼•
    å–æ¶ˆæ³¨é‡Šå¹¶è°ƒç”¨æ­¤å‡½æ•°æ¥ç´¢å¼•ä½ çš„ PDF æ–‡æ¡£
    """
    pdf_path = "arkUIè‡ªå®šä¹‰ç»„ä»¶ç”Ÿå‘½å‘¨æœŸ.pdf"
    persist_dir = CONFIG["vector_store_path"]
    
    if os.path.exists(persist_dir):
        response = input(f"âš ï¸  å‘é‡åº“å·²å­˜åœ¨äº {persist_dir}ï¼Œæ˜¯å¦é‡æ–°ç´¢å¼•ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("âŒ å–æ¶ˆç´¢å¼•æ“ä½œ")
            return
    
    vectorstore = load_and_index_pdf(pdf_path, persist_dir)
    print("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆï¼")


# ==================== å…¥å£ç‚¹ ====================
if __name__ == "__main__":
    # é¦–æ¬¡è¿è¡Œæ—¶å–æ¶ˆæ³¨é‡Šä»¥ä¸‹è¡Œæ¥åˆ›å»ºç´¢å¼•ï¼š
    # index_pdf_documents()
    
    # æ­£å¸¸è¿è¡Œåˆ†æï¼š
    main()